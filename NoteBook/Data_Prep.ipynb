{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7uvLXeMZ_Wb"
      },
      "source": [
        "# Unzip File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMb9LhIu7Ay7",
        "outputId": "99ec7598-0383-49d6-844f-0687d12331a4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSBPGfWZqDKG",
        "outputId": "562ddde6-9285-4c0f-de12-f256bd6829c1"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/My Drive/OMSCS/OMSCS_DL_Project/GenImage/BigGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40cadI86qNou",
        "outputId": "0508d8e4-9728-4596-f6e9-b71c57c224d8"
      },
      "outputs": [],
      "source": [
        "! ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgNvZLEnZ33W",
        "outputId": "ca892ad3-34f6-4312-ec3b-b8f4d0dda5c6"
      },
      "outputs": [],
      "source": [
        "! pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wevYIC50G1j",
        "outputId": "0dcb592c-af3c-4ed5-b78f-7c186047b5f3"
      },
      "outputs": [],
      "source": [
        "! ls /content/sample_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtSHPwnzqPNs",
        "outputId": "18c52198-10c2-4cca-8d34-87e312fe6535"
      },
      "outputs": [],
      "source": [
        "! unzip unsplit.zip -d /content/sample_data/BigGAN/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5er1YKCBrc9y"
      },
      "outputs": [],
      "source": [
        "# %cp -r /content/sample_data/BigGAN/ /content/drive/MyDrive/OMSCS_DL_Project/GenImage/BigGAN/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbMMAIrErtJB"
      },
      "outputs": [],
      "source": [
        "# ! zip -F imagenet_ai_0508_adm.zip --out unsplit.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLXsVhCSaImZ"
      },
      "source": [
        "## Check Number of files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O95pqc8-bNXM",
        "outputId": "e2a4e9cf-d536-4150-e3e7-1988dd1e6442"
      },
      "outputs": [],
      "source": [
        "! ls /content/sample_data/BigGAN/imagenet_ai_0419_biggan/train/ai | wc -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6rVaYYTcUb0",
        "outputId": "d950b97a-99f9-4066-8f4c-312f9c92e5c0"
      },
      "outputs": [],
      "source": [
        "! ls /content/sample_data/BigGAN/imagenet_ai_0419_biggan/train/nature/ | wc -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwZXZee3cigE",
        "outputId": "4cee6191-f87a-4582-c8f9-81e460bd5aac"
      },
      "outputs": [],
      "source": [
        "! ls /content/sample_data/BigGAN/imagenet_ai_0419_biggan/val/ai | wc -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_v3uZ2actGw",
        "outputId": "a64ee243-4839-4412-cd41-42cfcfb12a6e"
      },
      "outputs": [],
      "source": [
        "! ls /content/sample_data/BigGAN/imagenet_ai_0419_biggan/val/nature/ | wc -l"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecZcRwPIaUZM"
      },
      "source": [
        "Show Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "g7oXOxi1aTzR",
        "outputId": "a2aa1fdd-67cc-4910-9553-f6ae639eeb5d"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "plt.figure()\n",
        "img = mpimg.imread('/content/sample_data/BigGAN/imagenet_ai_0419_biggan/train/nature/n01582220_4551.JPEG')\n",
        "imgplot = plt.imshow(img)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJrkRVKKaMO2"
      },
      "source": [
        "# Prepare Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2BTr3haAvis"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from torchvision.io import read_image\n",
        "\n",
        "import torch\n",
        "\n",
        "from skimage import io, transform\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClNOPgqxgEid"
      },
      "outputs": [],
      "source": [
        "dataset_type = 'val'\n",
        "model_type = 'nature'\n",
        "root_dir = '/content/sample_data/BigGAN/imagenet_ai_0419_biggan'\n",
        "\n",
        "image_name = os.listdir(os.path.join(root_dir, dataset_type, model_type))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "MObCxRVikn1c",
        "outputId": "c4b41b90-657a-4a4b-a2d0-35c3ba950033"
      },
      "outputs": [],
      "source": [
        "image_name = os.path.join(root_dir, dataset_type, model_type,image_name[100])\n",
        "plt.figure()\n",
        "img = mpimg.imread(image_name)\n",
        "imgplot = plt.imshow(img)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqHeHrUm-QpH",
        "outputId": "d1e5c3f4-ddef-4b4a-ab82-6e353f930e78"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/My Drive/OMSCS/OMSCS_DL_Project/Deep_Learning_Final_Project/Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZFIQI_xVT3n",
        "outputId": "31ece8c7-9eb2-4e70-d20f-8e4ca6fea038"
      },
      "outputs": [],
      "source": [
        "! ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAm0pck3RgZo"
      },
      "outputs": [],
      "source": [
        "from data_prep_util import GenImageDataset, Rescale, HighPassConvLayer, State, CheckPoint\n",
        "from Entropy import Entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Define Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch_size: 32\n",
            "num_workers: 8\n",
            "num_epochs: 4\n",
            "learning_rate: 3e-05\n",
            "hidden_dim1: 500\n",
            "hidden_dim2: 500\n",
            "dropout: 0.1\n",
            "use_filter: True\n",
            "alpha_value: 0.5\n",
            "pretrained: False\n",
            "use_entropy_filter: False\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters and configurations\n",
        "class Config:\n",
        "    # for data loader\n",
        "    batch_size = 32\n",
        "    num_workers = 8\n",
        "\n",
        "    # number of epochs during training\n",
        "    num_epochs = 4\n",
        "\n",
        "    # learning rate for learnable parameters\n",
        "    learning_rate = 3e-5\n",
        "\n",
        "    # Define an MLP with 2 or 3 layers\n",
        "    hidden_dim1 = 500\n",
        "    hidden_dim2 = 500\n",
        "\n",
        "    # dropout in head\n",
        "    dropout = 0.1\n",
        "\n",
        "    # Set to False to disable the high pass filter\n",
        "    use_filter = True\n",
        "\n",
        "    # Adjust alpha between 0 and 1 for the desired effect for the high pass filter\n",
        "    alpha_value = 0.5\n",
        "\n",
        "    # Set to True if you want to use pretrained weights\n",
        "    pretrained = False\n",
        "\n",
        "    # Set to True if you want to use the entropy filter\n",
        "    use_entropy_filter = False\n",
        "\n",
        "    def print_values():\n",
        "        print('batch_size:', Config.batch_size)\n",
        "        print('num_workers:', Config.num_workers)\n",
        "        print('num_epochs:', Config.num_epochs)\n",
        "        print('learning_rate:', Config.learning_rate)\n",
        "        print('hidden_dim1:', Config.hidden_dim1)\n",
        "        print('hidden_dim2:', Config.hidden_dim2)\n",
        "        print('dropout:', Config.dropout)\n",
        "        print('use_filter:', Config.use_filter)\n",
        "        print('alpha_value:', Config.alpha_value)\n",
        "        print('pretrained:', Config.pretrained)\n",
        "        print('use_entropy_filter:', Config.use_entropy_filter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Define Transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_transformations(rescale_size=256):\n",
        "    transformations = [Rescale(rescale_size)]\n",
        "    return transforms.Compose(transformations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUkzR-5pnYIO"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset_type = 'train'\n",
        "# model_type = 'nature'\n",
        "root_dir = '/content/sample_data/BigGAN/imagenet_ai_0419_biggan'\n",
        "\n",
        "train_nature = GenImageDataset(root_dir, dataset_type, 'nature',\n",
        "                                      transform=get_transformations(), input_type='Image')\n",
        "\n",
        "train_ai = GenImageDataset(root_dir, dataset_type, 'ai',\n",
        "                                  transform=get_transformations(), input_type='Image')\n",
        "\n",
        "train = torch.utils.data.ConcatDataset(\n",
        "    [train_nature, train_ai])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vofDdgnK1Rs"
      },
      "outputs": [],
      "source": [
        "dataset_type = 'val'\n",
        "\n",
        "val_nature = GenImageDataset(root_dir, dataset_type, 'nature',\n",
        "                                    transform=get_transformations(), input_type='Image')\n",
        "\n",
        "val_ai = GenImageDataset(root_dir, dataset_type, 'ai',\n",
        "                                transform=get_transformations(), input_type='Image')\n",
        "\n",
        "val = torch.utils.data.ConcatDataset([val_nature, val_ai])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUpiGVXsLCsd",
        "outputId": "8d4980ff-20e2-459c-b705-22930bc77166"
      },
      "outputs": [],
      "source": [
        "len(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcSOSP2vAK0r",
        "outputId": "28f52f96-24ac-471a-a796-68ca8ad93429"
      },
      "outputs": [],
      "source": [
        "len(val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZ7aCLJ6M6lK",
        "outputId": "baf27ce6-25d9-477f-b947-1af96c37acb6"
      },
      "outputs": [],
      "source": [
        "train[100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-vZ1-_0n9zA",
        "outputId": "8ff1c8b2-c8de-4769-f3b1-9e1f0696e1f4"
      },
      "outputs": [],
      "source": [
        "train[100]['image'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "zFkATzPQnkL7",
        "outputId": "b9543e2b-f0cc-43dc-ac80-b28b06dc2988"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "img = train[100]['image']\n",
        "imgplot = plt.imshow(img.permute(1, 2, 0))\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEPrXJzT78z5"
      },
      "outputs": [],
      "source": [
        "next(iter(train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvbSxHNbF4TH"
      },
      "source": [
        "https://discuss.pytorch.org/t/dataloader-resets-dataset-state/27960\n",
        "\n",
        "https://discuss.pytorch.org/t/pytorch-dataloaders-in-memory/118471"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGZwGk5FAVVA"
      },
      "source": [
        "# Define Swin Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OA7aihvgBbtD"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xX-Vydb5AUlu",
        "outputId": "5728a995-c14d-43ae-f19d-5aa8dd52fb82"
      },
      "outputs": [],
      "source": [
        "# Wrapper for Swin Transformer to allow optional conv layer\n",
        "class CustomSwinModel(nn.Module):\n",
        "    def __init__(self, base_model, use_high_pass_filter=False):\n",
        "        super(CustomSwinModel, self).__init__()\n",
        "        self.use_high_pass_filter = use_high_pass_filter\n",
        "        self.high_pass_filter = HighPassConvLayer(\n",
        "        ) if use_high_pass_filter else nn.Identity()\n",
        "        self.base_model = base_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_high_pass_filter:\n",
        "            x = self.high_pass_filter(x)\n",
        "        return self.base_model(x)\n",
        "\n",
        "\n",
        "# Initialize the Swin Transformer model without pretrained weights\n",
        "# Notice pretrained is set to False\n",
        "base_model = models.swin_t(pretrained=Config.pretrained)\n",
        "\n",
        "# Create the custom model with the high-pass filter layer\n",
        "model = CustomSwinModel(base_model, use_high_pass_filter=Config.use_filter)\n",
        "\n",
        "\n",
        "# Move the model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "'''\n",
        "For transfer learning, uncomment to freeze the pretrained weights\n",
        "'''\n",
        "# for param in model.parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "classes = ['ai', 'nature']\n",
        "\n",
        "mlp_head = nn.Sequential(\n",
        "    nn.Linear(model.base_model.head.in_features, Config.hidden_dim1),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(Config.dropout),\n",
        "    nn.Linear(Config.hidden_dim1, Config.hidden_dim2),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(Config.dropout),\n",
        "    nn.Linear(Config.hidden_dim2, len(classes))\n",
        ").to(device)\n",
        "\n",
        "# Update the classifier head of the base_model inside CustomSwinModel\n",
        "model.base_model.head = mlp_head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2XpTEAHA5s3"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=Config.learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def accuracy(predictions, labels):\n",
        "    _, preds = torch.max(predictions, 1)\n",
        "    return (preds == labels).float().mean().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvSNiSRCBqlY"
      },
      "outputs": [],
      "source": [
        "trainloader = DataLoader(train, batch_size=Config.batch_size, shuffle=True)\n",
        "testloader = DataLoader(val, batch_size=Config.batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment 1: HighPass Filter \n",
        "\n",
        "> *the following code block contains visualizations related to experiment #1*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.fft import fft2, fftshift\n",
        "\n",
        "\n",
        "def apply_high_pass_filter(image, filter_layer):\n",
        "    # Convert the image to a floating-point format and add a batch dimension\n",
        "    image_batch = image.unsqueeze(0).to(torch.float32)\n",
        "\n",
        "    # Apply the high pass filter\n",
        "    filtered_image_batch = filter_layer(image_batch)\n",
        "\n",
        "    # Remove the batch dimension\n",
        "    filtered_image = filtered_image_batch.squeeze(0)\n",
        "\n",
        "    return filtered_image\n",
        "\n",
        "\n",
        "def plot_frequency_spectrum(image, title):\n",
        "    # Convert the image to grayscale if it's not already\n",
        "    if image.shape[0] == 3:\n",
        "        image = image.mean(0)  # Average across the color channels\n",
        "\n",
        "    # Apply FFT\n",
        "    f_image = fft2(image)\n",
        "    fshift = fftshift(f_image)\n",
        "\n",
        "    # Calculate magnitude spectrum and use log scale for better visibility\n",
        "    magnitude_spectrum = torch.log(torch.abs(fshift) + 1)\n",
        "\n",
        "    # Display the spectrum\n",
        "    plt.imshow(magnitude_spectrum.numpy(), cmap='gray')\n",
        "    plt.title(title)\n",
        "    plt.axis('off')\n",
        "\n",
        "\n",
        "# Instantiate the high-pass filter layer\n",
        "high_pass_filter_layer = HighPassConvLayer().to(device)\n",
        "\n",
        "# Get an image from the dataset and convert to float if necessary\n",
        "original_image = train[0]['image']\n",
        "if original_image.dtype == torch.uint8:\n",
        "    original_image = original_image.to(torch.float32) / 255.\n",
        "\n",
        "# Apply the high pass filter\n",
        "filtered_image = apply_high_pass_filter(original_image, high_pass_filter_layer)\n",
        "\n",
        "# Convert filtered image to float if necessary (the filter should already output float, but just in case)\n",
        "if filtered_image.dtype == torch.uint8:\n",
        "    filtered_image = filtered_image.to(torch.float32) / 255.\n",
        "\n",
        "# Display the original and filtered images and their frequency spectrums\n",
        "plt.figure(figsize=(12, 12))\n",
        "\n",
        "# Original image\n",
        "plt.subplot(2, 2, 1)\n",
        "# Move to CPU for visualization\n",
        "plt.imshow(original_image.permute(1, 2, 0).cpu().numpy())\n",
        "plt.title(\"Original Image\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "# Frequency spectrum of the original image\n",
        "plt.subplot(2, 2, 2)\n",
        "plot_frequency_spectrum(original_image, \"Original Image Frequency Spectrum\")\n",
        "\n",
        "# Filtered image\n",
        "plt.subplot(2, 2, 3)\n",
        "# Move to CPU for visualization\n",
        "plt.imshow(filtered_image.permute(1, 2, 0).cpu().numpy())\n",
        "plt.title(\"Filtered Image\")\n",
        "plt.axis(\"off\")\n",
        "\n",
        "# Frequency spectrum of the filtered image\n",
        "plt.subplot(2, 2, 4)\n",
        "plot_frequency_spectrum(filtered_image, \"Filtered Image Frequency Spectrum\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment 2 Setup: Entropy Filter \n",
        "\n",
        "> *run the following block before the training loop to perform experiment 2*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "Config.use_filter = False\n",
        "Config.use_entropy_filter = True\n",
        "Config.pretrained = False\n",
        "Config.print_values()\n",
        "base_model = models.swin_t(pretrained=Config.pretrained)\n",
        "model = CustomSwinModel(base_model, use_high_pass_filter=Config.use_filter)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "num_classes = 2\n",
        "mlp_head = nn.Sequential(\n",
        "    nn.Linear(model.base_model.head.in_features, Config.hidden_dim1),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(Config.dropout),\n",
        "    nn.Linear(Config.hidden_dim1, Config.hidden_dim2),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(Config.dropout),\n",
        "    nn.Linear(Config.hidden_dim2, num_classes)\n",
        ").to(device)\n",
        "\n",
        "model.base_model.head = mlp_head\n",
        "optimizer = optim.Adam(model.parameters(), lr=Config.learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scaler = GradScaler()\n",
        "\n",
        "original_image = train[1]['image']\n",
        "plt.imshow(original_image.permute(1, 2, 0))\n",
        "plt.show()\n",
        "\n",
        "entropy_filtered = Entropy.entropy_for_image(None, original_image)\n",
        "plt.imshow(entropy_filtered[0], cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDRxlY9qBUu1",
        "outputId": "85c25427-8f16-4463-9c6e-2d35d04ea29a"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from tqdm import tqdm\n",
        "import time  # Import the time module\n",
        "\n",
        "scaler = GradScaler()  # Initialize the GradScaler\n",
        "\n",
        "train_loss_history, val_loss_history, train_acc_history, val_acc_history = {}, {}, {}, {}\n",
        "total_training_start = time.time()  # Record the start time of the total training\n",
        "starting_epoch = 0\n",
        "load_latest_model = False\n",
        "\n",
        "state = CheckPoint.load_checkpoint()\n",
        "if state is not None and load_latest_model:\n",
        "    print('LOADING FROM CHECKPOINT')\n",
        "    model.load_state_dict(state.model_state_dict)\n",
        "    starting_epoch = state.epoch + 1\n",
        "    trainloader = state.trainloader\n",
        "    testloader = state.testloader\n",
        "    train_loss_history = state.train_loss_history\n",
        "    train_acc_history = state.train_acc_history\n",
        "    val_loss_history = state.val_loss_history\n",
        "    val_acc_history = state.val_acc_history\n",
        "    criterion.load_state_dict(state.criterion_state_dict)\n",
        "    optimizer.load_state_dict(state.optimizer_state_dict)\n",
        "    scaler.load_state_dict(state.scaler_state_dict)\n",
        "    print('starting_epoch', starting_epoch)\n",
        "    print('train_loss_history', train_loss_history)\n",
        "    print('train_acc_history', train_acc_history)\n",
        "    print('val_loss_history', val_loss_history)\n",
        "    print('val_acc_history', val_acc_history)\n",
        "\n",
        "for epoch in range(starting_epoch, Config.num_epochs):\n",
        "    epoch_start = time.time()  # Record the start time of the epoch\n",
        "\n",
        "    train_loss, train_acc, val_loss, val_acc = 0.0, 0.0, 0.0, 0.0\n",
        "    train_loss_history[epoch] = []\n",
        "    val_loss_history[epoch] = []\n",
        "    train_acc_history[epoch] = []\n",
        "    val_acc_history[epoch] = []\n",
        "\n",
        "    # Training Phase\n",
        "    model.train()\n",
        "    pbar = tqdm(enumerate(trainloader), total=len(trainloader),\n",
        "                desc=f\"Epoch {epoch+1} TRAIN\", ncols=100)\n",
        "    for i, data in pbar:\n",
        "        inputs = data['image']\n",
        "        if Config.use_entropy_filter:\n",
        "            numpy_inputs = inputs.numpy()\n",
        "            processed_inputs = []\n",
        "            for image in numpy_inputs:\n",
        "                processed_inputs.append(Entropy.entropy_for_image(None, image))\n",
        "            inputs = torch.stack(processed_inputs)\n",
        "        \n",
        "        inputs = inputs.to(torch.float)\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        labels = data['model_type']\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        dataset = data['dataset_type']\n",
        "        image_name = data['image_name']\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast():\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        acc = accuracy(outputs, labels)\n",
        "        train_loss_history[epoch].append(loss.item())\n",
        "        train_acc_history[epoch].append(acc)\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        train_acc += acc\n",
        "\n",
        "        pbar.set_description(f\"Epoch {epoch+1} TRAIN Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Validation Phase\n",
        "    model.eval()\n",
        "    pbar = tqdm(enumerate(testloader), total=len(testloader),\n",
        "                desc=f\"Epoch {epoch+1} VAL\", ncols=100)\n",
        "    with torch.no_grad():\n",
        "        for i, data in pbar:\n",
        "            inputs = data['image']\n",
        "            inputs = inputs.to(torch.float)\n",
        "            inputs = inputs.to(device)\n",
        "\n",
        "            labels = data['model_type']\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            dataset = data['dataset_type']\n",
        "            image_name = data['image_name']\n",
        "\n",
        "            # inputs, labels, dataset = inputs.cuda(), labels.cuda(), dataset.cuda()\n",
        "\n",
        "            with autocast():\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            acc = accuracy(outputs, labels)\n",
        "            val_loss_history[epoch].append(loss.item())\n",
        "            val_acc_history[epoch].append(acc)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "            val_acc += acc\n",
        "\n",
        "            pbar.set_description(\n",
        "                f\"Epoch {epoch+1} VAL Loss: {loss.item():.4f}\")\n",
        "\n",
        "    epoch_end = time.time()  # Record the end time of the epoch\n",
        "    # Calculate the duration in minutes\n",
        "    epoch_duration = (epoch_end - epoch_start) / 60\n",
        "\n",
        "    train_loss /= len(trainloader)\n",
        "    train_acc /= len(trainloader)\n",
        "    val_loss /= len(testloader)\n",
        "    val_acc /= len(testloader)\n",
        "\n",
        "    print(f\"Epoch Summary {epoch+1}/{Config.num_epochs}\")\n",
        "    print(\n",
        "        f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc*100:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc*100:.2f}%\")\n",
        "    print(f\"Epoch Duration: {epoch_duration:.2f} minutes\")\n",
        "    print('-' * 60)\n",
        "\n",
        "    state = State(\n",
        "        model_state_dict=model.state_dict(),\n",
        "        epoch=epoch,\n",
        "        trainloader=trainloader,\n",
        "        testloader=testloader,\n",
        "        train_loss_history=train_loss_history,\n",
        "        train_acc_history=train_acc_history,\n",
        "        val_loss_history=val_loss_history,\n",
        "        val_acc_history=val_acc_history,\n",
        "        criterion_state_dict=criterion.state_dict(),\n",
        "        optimizer_state_dict=optimizer.state_dict(),\n",
        "        scaler_state_dict=scaler.state_dict(),\n",
        "    )\n",
        "    CheckPoint.save_checkpoint(state)\n",
        "\n",
        "total_training_end = time.time()  # Record the end time of the total training\n",
        "# Calculate the total duration in minutes\n",
        "total_training_duration = (total_training_end - total_training_start) / 60\n",
        "print('Finished Training')\n",
        "print(f\"Total Training Time: {total_training_duration:.2f} minutes\")\n",
        "\n",
        "# https://stackoverflow.com/questions/59129812/how-to-avoid-cuda-out-of-memory-in-pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plot Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "training_losses = []\n",
        "training_acc = []\n",
        "validation_losses = []\n",
        "validation_acc = []\n",
        "\n",
        "# combine loss maps into a single array\n",
        "for epoch in train_loss_history:\n",
        "  training_losses += train_loss_history[epoch]\n",
        "  training_acc += train_acc_history[epoch]\n",
        "  validation_losses += val_loss_history[epoch]\n",
        "  validation_acc += val_acc_history[epoch]\n",
        "  \n",
        "plt.plot(training_losses, label='train')\n",
        "plt.plot(validation_losses, label='validation')\n",
        "plt.legend(loc='upper left')\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('loss')\n",
        "plt.savefig('losses.png')\n",
        "\n",
        "plt.clf()\n",
        "\n",
        "plt.plot(training_acc, label='train')\n",
        "plt.plot(validation_acc, label='validation')\n",
        "plt.legend(loc='upper left')\n",
        "plt.xlabel('iteration')\n",
        "plt.ylabel('accuracy')\n",
        "plt.savefig('accuracies.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2dJuzwRn17T"
      },
      "outputs": [],
      "source": [
        "# from itertools import islice\n",
        "\n",
        "# for i,data in enumerate(trainloader):\n",
        "#   if i>5200:\n",
        "#     print(data['image'].shape)\n",
        "#     print(i)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "9ckNL2Hx0wkR",
        "outputId": "f5304f0a-900e-47a0-f299-8258e635ce5f"
      },
      "outputs": [],
      "source": [
        "# plots with errors\n",
        "# /content/sample_data/BigGAN/imagenet_ai_0419_biggan/train/ai/116_biggan_00098.png\n",
        "# /content/sample_data/BigGAN/imagenet_ai_0419_biggan/train/ai/116_biggan_00107.png\n",
        "plt.figure()\n",
        "img = mpimg.imread('/content/sample_data/BigGAN/imagenet_ai_0419_biggan/train/ai/116_biggan_00094.png')\n",
        "imgplot = plt.imshow(img)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "o0LIc4GvN--i",
        "outputId": "ab9d5e68-cc4d-4954-ae57-8865a58447a7"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "del model, inputs, labels, dataset, image_name\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7g2nN5iHwhq"
      },
      "outputs": [],
      "source": [
        "torch.cuda.memory_summary(device=None, abbreviated=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
